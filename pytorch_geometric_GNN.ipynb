{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44e795da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# matplotlib for plotting\n",
    "import matplotlib\n",
    "%matplotlib inline \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch for neural network stuff\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import BatchNorm1d, MSELoss # loss function, normalization\n",
    "import torch.optim as optim # optimizer\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# torch-geometric for the graph stuff ontop of pytorch \n",
    "from torch_geometric.nn import GCNConv,global_mean_pool\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import from_networkx, to_networkx\n",
    "\n",
    "import torch_geometric.nn as geom_nn\n",
    "import torch_geometric.data as geom_data\n",
    "\n",
    "# datasets \n",
    "from torch_geometric.datasets import TUDataset\n",
    "\n",
    "# pytorch lightning for automating training for us \n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "\n",
    "# tqdm for progress bars\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# see if we have a gpu\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.cuda(\"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412a02b6",
   "metadata": {},
   "source": [
    "### Load data and take a look\n",
    "We are loading the MUTAG dataset. See [here](https://paperswithcode.com/dataset/mutag) for more info. Basically, it is a dataset of compounds with the goal to predict some property.\n",
    "\n",
    "I am following [this](https://colab.research.google.com/github/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial7/GNN_overview.ipynb) tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d3b8c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data object: Data(edge_attr=[7442, 4], edge_index=[2, 7442], x=[3371, 7], y=[188])\n",
      "Length: 188\n",
      "Min label 0.0, Max label: 1.0\n",
      "Average label: 0.66\n"
     ]
    }
   ],
   "source": [
    "dataset = TUDataset(root='~/tmp/MUTAG', name='MUTAG')\n",
    "data = dataset[0]\n",
    "\n",
    "print(\"Data object:\", dataset.data)\n",
    "print(\"Length:\", len(dataset))\n",
    "print(f\"Min label {dataset.data.y.float().min().item()}, Max label: {dataset.data.y.float().max().item()}\")\n",
    "print(\"Average label: %4.2f\" % (dataset.data.y.float().mean().item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ac6eca",
   "metadata": {},
   "source": [
    "We have 188 samples (graphs) with the labels ranging from 0 to 1 and the average label being 0.66. We can print more information/plot them (exercise left to the reader :) )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1eb4c7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed for reproducibility \n",
    "torch.manual_seed(42)\n",
    "\n",
    "#shuffle the dataset so we can randomly sample our train/test split\n",
    "dataset = dataset.shuffle()\n",
    "\n",
    "# split train/test\n",
    "train_dataset = dataset[:150]\n",
    "test_dataset = dataset[150:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f9694b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a dataloader to load data\n",
    "graph_train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# train and validation sets should NEVER have the same data\n",
    "# will be doing it anyways because there is not the biggest dataset\n",
    "graph_val_loader = geom_data.DataLoader(test_dataset, batch_size=64) # Additional loader if you want to change to a larger dataset\n",
    "graph_test_loader = geom_data.DataLoader(test_dataset, batch_size=64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf65e9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: Batch(batch=[668], edge_attr=[1466, 4], edge_index=[2, 1466], ptr=[39], x=[668, 7], y=[38])\n",
      "Labels: tensor([1, 0, 1, 0, 0, 1, 1, 1, 1, 1])\n",
      "Batch indices: tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(graph_test_loader))\n",
    "\n",
    "\n",
    "# torch-geometric batches are composed of graphs but come in one Batch object.\n",
    "# we know which edge/node come from which graph because of the the edge_index attribute\n",
    "\n",
    "\n",
    "print(\"Batch:\", batch)\n",
    "print(\"Labels:\", batch.y[:10])\n",
    "print(\"Batch indices:\", batch.batch[:40])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac399f8",
   "metadata": {},
   "source": [
    "### Lets define our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84de86ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnn_layer_by_name = {\n",
    "#     \"GCN\": geom_nn.GCNConv,\n",
    "#     \"GAT\": geom_nn.GATConv,\n",
    "#     \"GraphConv\": geom_nn.GraphConv\n",
    "# }\n",
    "\n",
    "# regular GNN model (will be our base model)\n",
    "# The GNN applies a sequence of graph layers (GCN, GAT, or GraphConv),\n",
    "# ReLU (rectified linear unit) as activation function, and dropout for regularization.\n",
    "class GNNModel(nn.Module):    \n",
    "    def __init__(self, c_in, c_hidden, c_out, num_layers=2, dp_rate=0.1, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of the output features. Usually number of classes in classification\n",
    "            num_layers - Number of \"hidden\" graph layers\n",
    "            dp_rate - Dropout rate to apply throughout the network\n",
    "            kwargs - Additional arguments for the graph layer (e.g. number of heads for GAT)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        gnn_layer = geom_nn.GraphConv\n",
    "        \n",
    "        layers = []\n",
    "        in_channels, out_channels = c_in, c_hidden\n",
    "        for l_idx in range(num_layers-1):\n",
    "            layers += [\n",
    "                gnn_layer(in_channels=in_channels, \n",
    "                          out_channels=out_channels,\n",
    "                          **kwargs),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dp_rate)\n",
    "            ]\n",
    "            in_channels = c_hidden\n",
    "        layers += [gnn_layer(in_channels=in_channels, \n",
    "                             out_channels=c_out,\n",
    "                             **kwargs)]\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        \"\"\"\n",
    "        Forward-pass function. What gets called when we invoke our model (aka predict)\n",
    "        ie GNNModel(*params)(batch)\n",
    "        \n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "        \"\"\"\n",
    "        for l in self.layers:\n",
    "            # For graph layers, we need to add the \"edge_index\" tensor as additional input\n",
    "            # All PyTorch Geometric graph layer inherit the class \"MessagePassing\", hence\n",
    "            # we can simply check the class type.\n",
    "            \n",
    "            if isinstance(l, geom_nn.MessagePassing):\n",
    "                x = l(x, edge_index)\n",
    "            else:\n",
    "                x = l(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "    \n",
    "# with the graph structure we need to add a pooling layer since\n",
    "# the batch_idx parameter tells us which nodes belong to which graph\n",
    "# Here we simply use the above model but add an AveragePooling layer\n",
    "# this simple averages over node/edge attributes and extracts features into an array\n",
    "class GraphGNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, c_in, c_hidden, c_out, dp_rate_linear=0.5, **kwargs):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            c_in - Dimension of input features\n",
    "            c_hidden - Dimension of hidden features\n",
    "            c_out - Dimension of output features (usually number of classes)\n",
    "            dp_rate_linear - Dropout rate before the linear layer (usually much higher than inside the GNN)\n",
    "            kwargs - Additional arguments for the GNNModel object\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # use our model from above\n",
    "        \n",
    "        self.GNN = GNNModel(c_in=c_in, \n",
    "                            c_hidden=c_hidden, \n",
    "                            c_out=c_hidden, # Not our prediction output yet!\n",
    "                            **kwargs)\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Dropout(dp_rate_linear),\n",
    "            nn.Linear(c_hidden, c_out)\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, x, edge_index, batch_idx):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x - Input features per node\n",
    "            edge_index - List of vertex index pairs representing the edges in the graph (PyTorch geometric notation)\n",
    "            batch_idx - Index of batch element for each node\n",
    "        \"\"\"\n",
    "        x = self.GNN(x, edge_index)\n",
    "        x = geom_nn.global_mean_pool(x, batch_idx) # Average pooling\n",
    "        x = self.head(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365540ce",
   "metadata": {},
   "source": [
    "### Lets use a Pytorch-lighting model to handle the training for us\n",
    "\n",
    "From docs: \"PyTorch Lightning is just organized PyTorch\". \n",
    "It takes care of a lot for us. \n",
    "\n",
    "For more info on how to use lightning models, see [torch-lightning github](https://github.com/PyTorchLightning/pytorch-lightning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12496680",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphLevelGNN(pl.LightningModule):\n",
    "    def __init__(self, **model_kwargs):\n",
    "        super().__init__()\n",
    "        # Saving hyperparameters\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        self.model = GraphGNNModel(**model_kwargs)\n",
    "        self.loss_module = nn.BCEWithLogitsLoss() if self.hparams.c_out == 1 else nn.CrossEntropyLoss()\n",
    "        \n",
    "    # forward in torch lightning module is not the same as before. \n",
    "    # here it means the prediction step\n",
    "    def forward(self, data, mode=\"train\"):\n",
    "        x, edge_index, batch_idx = data.x, data.edge_index, data.batch\n",
    "        x = self.model(x, edge_index, batch_idx)\n",
    "        x = x.squeeze(dim=-1)\n",
    "        \n",
    "        if self.hparams.c_out == 1:\n",
    "            preds = (x > 0).float()\n",
    "            data.y = data.y.float()\n",
    "        else:\n",
    "            preds = x.argmax(dim=-1)\n",
    "        loss = self.loss_module(x, data.y)\n",
    "        acc = (preds == data.y).sum().float() / preds.shape[0]\n",
    "        return loss, acc\n",
    "        \n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.parameters(), lr=1e-2, weight_decay=0.0) # High lr because of small dataset and small model\n",
    "        return optimizer\n",
    "        \n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, acc = self.forward(batch, mode=\"train\")\n",
    "        self.log('train_loss', loss)\n",
    "        self.log('train_acc', acc)\n",
    "        return loss\n",
    "        \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"val\")\n",
    "        self.log('val_acc', acc)\n",
    "        \n",
    "        \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, acc = self.forward(batch, mode=\"test\")\n",
    "        self.log('test_acc', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d60830d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to train \n",
    "def train_graph_classifier(model_name, **model_kwargs):\n",
    "    pl.seed_everything(42)\n",
    "    \n",
    "    # Create a PyTorch Lightning trainer with the generation callback\n",
    "    root_dir = os.path.join(CHECKPOINT_PATH, \"GraphLevel\" + model_name)\n",
    "    os.makedirs(root_dir, exist_ok=True)\n",
    "    \n",
    "    trainer = pl.Trainer(default_root_dir=root_dir,\n",
    "                         callbacks=[ModelCheckpoint(save_weights_only=True, mode=\"max\", monitor=\"val_acc\")],\n",
    "                         gpus=1 if str(device).startswith(\"cuda\") else 0,\n",
    "                         max_epochs=500,\n",
    "                         progress_bar_refresh_rate=0)\n",
    "    \n",
    "    trainer.logger._default_hp_metric = None # Optional logging argument that we don't need\n",
    "\n",
    "    # Check whether pretrained model exists. If yes, load it and skip training\n",
    "    pretrained_filename = os.path.join(CHECKPOINT_PATH, \"GraphLevel%s.ckpt\" % model_name)\n",
    "    if os.path.isfile(pretrained_filename):\n",
    "        print(\"Found pretrained model, loading...\")\n",
    "        model = GraphLevelGNN.load_from_checkpoint(pretrained_filename)\n",
    "    else:\n",
    "        pl.seed_everything(42)\n",
    "        model = GraphLevelGNN(c_in=dataset.num_node_features, \n",
    "                              c_out=1 if dataset.num_classes==2 else dataset.num_classes, \n",
    "                              **model_kwargs)\n",
    "        trainer.fit(model, graph_train_loader, graph_val_loader)\n",
    "        model = GraphLevelGNN.load_from_checkpoint(trainer.checkpoint_callback.best_model_path)\n",
    "    # Test best model on validation and test set\n",
    "    train_result = trainer.test(model, test_dataloaders=graph_train_loader, verbose=False)\n",
    "    test_result = trainer.test(model, test_dataloaders=graph_test_loader, verbose=False)\n",
    "    result = {\"test\": test_result[0]['test_acc'], \"train\": train_result[0]['test_acc']} \n",
    "    return model, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bc269e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "Global seed set to 42\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name        | Type              | Params\n",
      "--------------------------------------------------\n",
      "0 | model       | GraphGNNModel     | 266 K \n",
      "1 | loss_module | BCEWithLogitsLoss | 0     \n",
      "--------------------------------------------------\n",
      "266 K     Trainable params\n",
      "0         Non-trainable params\n",
      "266 K     Total params\n",
      "1.067     Total estimated model params size (MB)\n",
      "/h/ibenlo/miniconda3/envs/fujitsu/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "Global seed set to 42\n",
      "/h/ibenlo/miniconda3/envs/fujitsu/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/h/ibenlo/miniconda3/envs/fujitsu/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: Your test_dataloader has `shuffle=True`, it is best practice to turn this off for val/test/predict dataloaders.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/h/ibenlo/miniconda3/envs/fujitsu/lib/python3.7/site-packages/pytorch_lightning/utilities/distributed.py:69: UserWarning: The dataloader, test dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 32 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = './checkpoints'\n",
    "model, result = train_graph_classifier(model_name=\"GraphConv\", \n",
    "                                       c_hidden=256, \n",
    "                                       num_layers=3, \n",
    "                                       dp_rate_linear=0.5,\n",
    "                                       dp_rate=0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "184a1c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train performance: 82.10%\n",
      "Test performance:  86.84%\n"
     ]
    }
   ],
   "source": [
    "print(\"Train performance: %4.2f%%\" % (100.0*result['train']))\n",
    "print(\"Test performance:  %4.2f%%\" % (100.0*result['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee1da08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
