{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89a41c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation of reinforce in pytorch. \n",
    "# See p. 326 Reinforcement Learning 2nd ed. Sutton & Barto \n",
    "# By Andriy Drozdyuk.\n",
    "import torch\n",
    "import gym\n",
    "\n",
    "step = 0\n",
    "max_steps = 10_000\n",
    "lr = 0.005\n",
    "γ = 0.9999\n",
    "\n",
    "# %%\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "nn = torch.nn.Sequential(\n",
    "    torch.nn.Linear(4, 64),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(64, env.action_space.n),\n",
    "    torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "optim = torch.optim.Adam(nn.parameters(), lr=lr)\n",
    "\n",
    "assert isinstance(nn(torch.tensor(env.reset(), dtype=torch.float)), torch.Tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ad424d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 14: Reward=14.0\n",
      "Step: 42: Reward=28.0\n",
      "Step: 56: Reward=14.0\n",
      "Step: 67: Reward=11.0\n",
      "Step: 84: Reward=17.0\n",
      "Step: 112: Reward=28.0\n",
      "Step: 153: Reward=41.0\n",
      "Step: 175: Reward=22.0\n",
      "Step: 192: Reward=17.0\n",
      "Step: 210: Reward=18.0\n",
      "Step: 225: Reward=15.0\n",
      "Step: 240: Reward=15.0\n",
      "Step: 259: Reward=19.0\n",
      "Step: 277: Reward=18.0\n",
      "Step: 304: Reward=27.0\n",
      "Step: 321: Reward=17.0\n",
      "Step: 355: Reward=34.0\n",
      "Step: 394: Reward=39.0\n",
      "Step: 416: Reward=22.0\n",
      "Step: 438: Reward=22.0\n",
      "Step: 460: Reward=22.0\n",
      "Step: 500: Reward=40.0\n",
      "Step: 523: Reward=23.0\n",
      "Step: 565: Reward=42.0\n",
      "Step: 588: Reward=23.0\n",
      "Step: 608: Reward=20.0\n",
      "Step: 629: Reward=21.0\n",
      "Step: 667: Reward=38.0\n",
      "Step: 684: Reward=17.0\n",
      "Step: 743: Reward=59.0\n",
      "Step: 798: Reward=55.0\n",
      "Step: 822: Reward=24.0\n",
      "Step: 861: Reward=39.0\n",
      "Step: 909: Reward=48.0\n",
      "Step: 984: Reward=75.0\n",
      "Step: 1018: Reward=34.0\n",
      "Step: 1044: Reward=26.0\n",
      "Step: 1070: Reward=26.0\n",
      "Step: 1088: Reward=18.0\n",
      "Step: 1139: Reward=51.0\n",
      "Step: 1167: Reward=28.0\n",
      "Step: 1191: Reward=24.0\n",
      "Step: 1239: Reward=48.0\n",
      "Step: 1264: Reward=25.0\n",
      "Step: 1304: Reward=40.0\n",
      "Step: 1330: Reward=26.0\n",
      "Step: 1359: Reward=29.0\n",
      "Step: 1375: Reward=16.0\n",
      "Step: 1409: Reward=34.0\n",
      "Step: 1440: Reward=31.0\n",
      "Step: 1467: Reward=27.0\n",
      "Step: 1514: Reward=47.0\n",
      "Step: 1562: Reward=48.0\n",
      "Step: 1601: Reward=39.0\n",
      "Step: 1633: Reward=32.0\n",
      "Step: 1647: Reward=14.0\n",
      "Step: 1679: Reward=32.0\n",
      "Step: 1752: Reward=73.0\n",
      "Step: 1776: Reward=24.0\n",
      "Step: 1807: Reward=31.0\n",
      "Step: 1855: Reward=48.0\n",
      "Step: 1880: Reward=25.0\n",
      "Step: 1981: Reward=101.0\n",
      "Step: 2073: Reward=92.0\n",
      "Step: 2115: Reward=42.0\n",
      "Step: 2181: Reward=66.0\n",
      "Step: 2215: Reward=34.0\n",
      "Step: 2277: Reward=62.0\n",
      "Step: 2398: Reward=121.0\n",
      "Step: 2458: Reward=60.0\n",
      "Step: 2492: Reward=34.0\n",
      "Step: 2532: Reward=40.0\n",
      "Step: 2623: Reward=91.0\n",
      "Step: 2665: Reward=42.0\n",
      "Step: 2715: Reward=50.0\n",
      "Step: 2753: Reward=38.0\n",
      "Step: 2830: Reward=77.0\n",
      "Step: 2851: Reward=21.0\n",
      "Step: 2889: Reward=38.0\n",
      "Step: 2951: Reward=62.0\n",
      "Step: 2987: Reward=36.0\n",
      "Step: 3116: Reward=129.0\n",
      "Step: 3159: Reward=43.0\n",
      "Step: 3217: Reward=58.0\n",
      "Step: 3252: Reward=35.0\n",
      "Step: 3333: Reward=81.0\n",
      "Step: 3388: Reward=55.0\n",
      "Step: 3446: Reward=58.0\n",
      "Step: 3538: Reward=92.0\n",
      "Step: 3603: Reward=65.0\n",
      "Step: 3760: Reward=157.0\n",
      "Step: 3803: Reward=43.0\n",
      "Step: 3835: Reward=32.0\n",
      "Step: 3907: Reward=72.0\n",
      "Step: 3960: Reward=53.0\n",
      "Step: 4045: Reward=85.0\n",
      "Step: 4088: Reward=43.0\n",
      "Step: 4148: Reward=60.0\n",
      "Step: 4218: Reward=70.0\n",
      "Step: 4341: Reward=123.0\n",
      "Step: 4464: Reward=123.0\n",
      "Step: 4539: Reward=75.0\n",
      "Step: 4689: Reward=150.0\n",
      "Step: 4751: Reward=62.0\n",
      "Step: 4843: Reward=92.0\n",
      "Step: 4869: Reward=26.0\n",
      "Step: 4939: Reward=70.0\n",
      "Step: 4980: Reward=41.0\n",
      "Step: 5053: Reward=73.0\n",
      "Step: 5100: Reward=47.0\n",
      "Step: 5166: Reward=66.0\n",
      "Step: 5213: Reward=47.0\n",
      "Step: 5252: Reward=39.0\n",
      "Step: 5294: Reward=42.0\n",
      "Step: 5340: Reward=46.0\n",
      "Step: 5397: Reward=57.0\n",
      "Step: 5462: Reward=65.0\n",
      "Step: 5496: Reward=34.0\n",
      "Step: 5528: Reward=32.0\n",
      "Step: 5559: Reward=31.0\n",
      "Step: 5617: Reward=58.0\n",
      "Step: 5654: Reward=37.0\n",
      "Step: 5690: Reward=36.0\n",
      "Step: 5718: Reward=28.0\n",
      "Step: 5761: Reward=43.0\n",
      "Step: 5802: Reward=41.0\n",
      "Step: 5838: Reward=36.0\n",
      "Step: 5887: Reward=49.0\n",
      "Step: 5920: Reward=33.0\n",
      "Step: 5967: Reward=47.0\n",
      "Step: 5994: Reward=27.0\n",
      "Step: 6039: Reward=45.0\n",
      "Step: 6125: Reward=86.0\n",
      "Step: 6184: Reward=59.0\n",
      "Step: 6212: Reward=28.0\n",
      "Step: 6285: Reward=73.0\n",
      "Step: 6362: Reward=77.0\n",
      "Step: 6446: Reward=84.0\n",
      "Step: 6573: Reward=127.0\n",
      "Step: 6640: Reward=67.0\n",
      "Step: 6720: Reward=80.0\n",
      "Step: 6820: Reward=100.0\n",
      "Step: 6944: Reward=124.0\n",
      "Step: 7088: Reward=144.0\n",
      "Step: 7188: Reward=100.0\n",
      "Step: 7356: Reward=168.0\n",
      "Step: 7556: Reward=200.0\n",
      "Step: 7687: Reward=131.0\n",
      "Step: 7887: Reward=200.0\n",
      "Step: 8087: Reward=200.0\n",
      "Step: 8287: Reward=200.0\n",
      "Step: 8418: Reward=131.0\n",
      "Step: 8618: Reward=200.0\n",
      "Step: 8818: Reward=200.0\n",
      "Step: 9018: Reward=200.0\n",
      "Step: 9218: Reward=200.0\n",
      "Step: 9338: Reward=120.0\n",
      "Step: 9538: Reward=200.0\n",
      "Step: 9733: Reward=195.0\n",
      "Step: 9858: Reward=125.0\n",
      "Step: 10043: Reward=185.0\n"
     ]
    }
   ],
   "source": [
    "while step < max_steps:\n",
    "    obs = torch.tensor(env.reset(), dtype=torch.float)    \n",
    "    done = False\n",
    "    Actions, States, Rewards, EligibilityVector = [], [], [], []\n",
    "    \n",
    "    while not done:\n",
    "        probs = nn(obs)\n",
    "        c = torch.distributions.Categorical(probs=probs)        \n",
    "        action = c.sample()        \n",
    "        log_prob = c.log_prob(action)        \n",
    "        action = action.item()\n",
    "        \n",
    "        assert isinstance(action, int)\n",
    "        obs_, rew, done, _info = env.step(action)\n",
    "        step += 1\n",
    "        \n",
    "        Actions.append(action)\n",
    "        States.append(obs)\n",
    "        Rewards.append(rew)\n",
    "        EligibilityVector.append(log_prob)\n",
    "\n",
    "        obs = torch.tensor(obs_, dtype=torch.float)\n",
    "\n",
    "    DiscountedReturns = []\n",
    "    for t in range(len((Rewards))):\n",
    "        G = 0.0\n",
    "        for k, r in enumerate(Rewards[t:]):\n",
    "            G += (γ**k)*r\n",
    "        G = (γ**t)*G\n",
    "        DiscountedReturns.append(G)\n",
    "    \n",
    "    EligibilityVector = torch.stack(EligibilityVector)        \n",
    "    DiscountedReturns = torch.tensor(DiscountedReturns, dtype=torch.float)\n",
    "    \n",
    "    assert EligibilityVector.shape == (len(Actions),)\n",
    "    assert DiscountedReturns.shape == (len(Actions),)\n",
    "\n",
    "    loss = - torch.dot(EligibilityVector, DiscountedReturns)\n",
    "    \n",
    "    assert loss.dim() == 0\n",
    "    \n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "    print(f'Step: {step}: Reward={sum(Rewards)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3e840b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 168.0\n",
      "Reward: 123.0\n",
      "Reward: 182.0\n",
      "Reward: 200.0\n",
      "Reward: 119.0\n"
     ]
    }
   ],
   "source": [
    "for _ in range(5):\n",
    "    obs = torch.tensor(env.reset(), dtype=torch.float)    \n",
    "    done = False\n",
    "    env.render()\n",
    "    Rewards = []\n",
    "    while not done:\n",
    "        probs = nn(obs)\n",
    "        c = torch.distributions.Categorical(probs=probs)        \n",
    "        action = c.sample()            \n",
    "        action = action.item()\n",
    "        \n",
    "        obs_, rew, done, _info = env.step(action)\n",
    "        Rewards.append(rew)\n",
    "        env.render()\n",
    "\n",
    "        obs = torch.tensor(obs_, dtype=torch.float)\n",
    "\n",
    "    print(f'Reward: {sum(Rewards)}')\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b1da18",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
